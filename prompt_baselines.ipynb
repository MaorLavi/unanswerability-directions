{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba2a7c7",
   "metadata": {},
   "source": [
    "### save model's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea9fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.generate_directions import generate_directions\n",
    "from pipeline.model_utils.model_factory import construct_model_base\n",
    "from data.load_datasets import load_data\n",
    "\n",
    "model_name = \"llama3\"  # \"gemma3\" or \"llama3\"\n",
    "\n",
    "model_path = {\n",
    "    \"llama3\": \"meta-llama/meta-llama-3-8b-instruct\",\n",
    "    \"gemma3\": \"google/gemma-3-12b-it\"\n",
    "}[model_name]\n",
    "\n",
    "model_base = construct_model_base(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b78237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.load_datasets import load_data\n",
    "from evaluate import evaluate_by_completions\n",
    "import torch\n",
    "\n",
    "eval_data = \"nq\" # Choose eval dataset: \"squad\", \"repliqa\", \"nq\", \"musique\"\n",
    "prompt_type = \"standard_prompt\" # Choose prompt type: \"abstain_aware_prompt\", \"standard_prompt\"\n",
    "dir_data = f'data/{prompt_type}/{eval_data}'\n",
    "ans_test, unans_test = load_data(dir_data, \"test\")\n",
    "dirs_path = f'pipeline/runs/{model_name}/{eval_data}'\n",
    "evaluate_by_completions(f'{dirs_path}/evaluations/{prompt_type}/test/', ans_test, unans_test, model_base, dir_vector=None, layer=None, intervention_label = \"baseline\", batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8c1e2",
   "metadata": {},
   "source": [
    "### evaluate with GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66248fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def classify_completion(completion):\n",
    "    messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You will be shown a model's response to a question. Your task is to decide whether the model attempted to answer the question.\\n\"\n",
    "                        \"It is considered to abstain if it explicitly says it cannot answer the question, indicates uncertainty, or refuses to provide an answer because the context is insufficient.\\n\"\n",
    "                        \"Otherwise, it is considered to have answered the question.\\n\\n\"\n",
    "                        \"Respond with \\\"yes\\\" if the model attempted to answer the question.\\n\"\n",
    "                        \"Respond with \\\"no\\\" if the model abstained.\\n\"\n",
    "                        \"You will respond with a single word: 'yes' or 'no'. Do not provide any explanation or extra text.\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"1598\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"yes\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"abc\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"yes\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"The answer can be answered by the passage. According to the passage, Fort Coligny was destroyed in 1560.\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"yes\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"According to the passage, problems capable of theoretical solutions but consuming unreasonable time in practical application are known as intractable problems\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"yes\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"The passage does not mention BSkyB agreeing with any claim. In fact, it states that BSkyB denied Virgin Media\\'s claim that they had substantially increased the asking price for the channels. Therefore, the answer is \\\\\"unanswerable\\\\\".\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"no\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"The passage does not answer the question. The passage only mentions that the five counties (Los Angeles, Orange, San Diego, San Bernardino, and Riverside) are among the top 15 most populous counties in the United States, but it does not provide the actual names of the five most populous counties in the United States.\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"no\"},\n",
    "                {\"role\": \"user\", \"content\": 'Model response: \"The passage does not mention the birth year of Reguier de la Plancha. It only mentions that he died in 1560, but does not provide his birth year.\"\\nAnswer:'},\n",
    "                {\"role\": \"assistant\", \"content\": \"no\"},\n",
    "                {\"role\": \"user\", \"content\": f'Model response: \"{completion}\"\\nAnswer:'}\n",
    "            ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1\n",
    "    )\n",
    "    raw = response.choices[0].message.content.strip().lower()\n",
    "    if not raw or (raw != \"yes\" and raw != \"no\"):\n",
    "        raise ValueError(f\"Unexpected response from GPT: {raw}. Expected 'yes' or 'no'.\")\n",
    "    return 1 if raw.startswith(\"yes\") else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47be92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(tp, fp, fn):\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91398746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "results = {}\n",
    "for model_name in ['llama3', 'gemma3']:\n",
    "    for dataset in ['squad', 'repliqa', 'nq', 'musique']:\n",
    "        path = f'pipeline/runs/{model_name}/{dataset}/evaluations/{prompt_type}/completions/'\n",
    "        with open(f'{path}baseline_ans_completions.json', 'r') as f:\n",
    "            baseline_ans_completions = json.load(f)\n",
    "        with open(f'{path}baseline_unans_completions.json', 'r') as f:\n",
    "            baseline_unans_completions = json.load(f)\n",
    "        \n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "\n",
    "        for item in tqdm(baseline_ans_completions, desc=f\"{model_name}/{dataset}/answerable\"):\n",
    "            pred = classify_completion(item[\"response\"])\n",
    "            pred_labels.append(pred)\n",
    "            true_labels.append(1)\n",
    "\n",
    "        for item in tqdm(baseline_unans_completions, desc=f\"{model_name}/{dataset}/unanswerable\"):\n",
    "            pred = classify_completion(item[\"response\"])\n",
    "            pred_labels.append(pred)\n",
    "            true_labels.append(0)\n",
    "\n",
    "        tp_ans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 1 and t == 1)\n",
    "        fp_ans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 1 and t == 0)\n",
    "        fn_ans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 0 and t == 1)\n",
    "\n",
    "        tp_unans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 0 and t == 0)\n",
    "        fp_unans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 0 and t == 1)\n",
    "        fn_unans = sum(1 for p, t in zip(pred_labels, true_labels) if p == 1 and t == 0)\n",
    "\n",
    "        precision_ans, recall_ans, f1_ans = compute_metrics(tp_ans, fp_ans, fn_ans)\n",
    "        precision_unans, recall_unans, f1_unans = compute_metrics(tp_unans, fp_unans, fn_unans)\n",
    "        avg_f1 = (f1_ans + f1_unans) / 2\n",
    "\n",
    "        results[(model_name, dataset)] = {\n",
    "            \"precision_answered\": precision_ans,\n",
    "            \"recall_answered\": recall_ans,\n",
    "            \"f1_answered\": f1_ans,\n",
    "            \"precision_abstained\": precision_unans,\n",
    "            \"recall_abstained\": recall_unans,\n",
    "            \"f1_abstained\": f1_unans,\n",
    "            \"avg_f1\": avg_f1,\n",
    "        }\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "json_safe_results = {\n",
    "    f\"{model}:{dataset}\": metrics\n",
    "    for (model, dataset), metrics in results.items()\n",
    "}\n",
    "with open(f\"evaluations/{prompt_type}/gpt_classification_results.json\", \"w\") as f:\n",
    "    json.dump(json_safe_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b459674",
   "metadata": {},
   "source": [
    "### plot evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028248b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"evaluations/standard_prompt/gpt_classification_results.json\", \"r\") as f:\n",
    "    regular_results = json.load(f)\n",
    "\n",
    "with open(\"evaluations/abstain_aware_prompt/gpt_classification_results.json\", \"r\") as f:\n",
    "    unans_prompt_results = json.load(f)\n",
    "\n",
    "formal_dataset_names = {\n",
    "    \"squad\": \"SQuAD\",\n",
    "    \"nq\": \"NQ\",\n",
    "    \"musique\": \"MuSiQue\",\n",
    "    \"repliqa\": \"RepLiQA\"\n",
    "}\n",
    "\n",
    "formal_model_names = {\n",
    "    \"llama3\": \"Llama 3\",\n",
    "    \"gemma3\": \"Gemma 3\"\n",
    "}\n",
    "\n",
    "plot_data = []\n",
    "\n",
    "for method_name, results_dict in [\n",
    "    (\"St. Prompt\", regular_results),\n",
    "    (\"Abs. Prompt\", unans_prompt_results)\n",
    "]:\n",
    "    for key, value in results_dict.items():\n",
    "        model_key, dataset_key = key.split(\":\")\n",
    "        model_name = formal_model_names[model_key]\n",
    "        dataset_name = formal_dataset_names[dataset_key]\n",
    "        combined_label = f\"{method_name} â€“ {model_name}\"\n",
    "        plot_data.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Group\": combined_label,\n",
    "            \"Recall_Unanswerable\": value[\"recall_unans\"]*100,\n",
    "            \"Avg_F1\": value[\"avg_f1\"]*100\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(plot_data)\n",
    "\n",
    "df[\"Dataset\"] = pd.Categorical(df[\"Dataset\"], categories=[\"SQuAD\", \"RepLiQA\", \"NQ\", \"MuSiQue\"], ordered=True)\n",
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "sns.set_context(\"notebook\", font_scale=1.3)\n",
    "plt.figure(figsize=(9.5, 5))\n",
    "\n",
    "custom_palette = [\"#7a6bbf\", \"#ff9c42\", \"#5cb85c\", \"#e15759\"]\n",
    "sns.set_palette(custom_palette)\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Dataset\",\n",
    "    y = \"Avg_F1\",\n",
    "    hue=\"Group\",\n",
    "    dodge=True,\n",
    "    width=0.85\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Macro-Average F1 Score\", fontsize=20)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=22)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=18)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.1f\", label_type=\"edge\", padding=2, fontsize=14.5)\n",
    "\n",
    "plt.legend(title=\"\", loc=\"upper center\", fontsize=17, ncol = 2, bbox_to_anchor=(0.5, 1.3), frameon=False, labelspacing = 0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"plots/prompts_f1.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"plots/prompts_f1.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3e6cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "sns.set_context(\"notebook\", font_scale=1.3)\n",
    "plt.figure(figsize=(9.5, 5))\n",
    "\n",
    "custom_palette = [\"#7a6bbf\", \"#ff9c42\", \"#5cb85c\", \"#e15759\"]\n",
    "sns.set_palette(custom_palette)\n",
    "ax = sns.barplot(\n",
    "    data=df,\n",
    "    x=\"Dataset\",\n",
    "    y=\"Recall_Unanswerable\",\n",
    "    hue=\"Group\",\n",
    "    dodge=True,\n",
    "    width=0.85\n",
    ")\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Abstention Rate on\\nUnanswerable Questions\", fontsize=20)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=22)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=18)\n",
    "# Add bar labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.1f\", label_type=\"edge\", padding=2, fontsize=14.5)\n",
    "\n",
    "plt.legend(title=\"\", loc=\"upper center\", fontsize=17, ncol = 2, bbox_to_anchor=(0.5, 1.3), frameon=False, labelspacing = 0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(\"plots/recall_unans_prompts.pdf\", format=\"pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"plots/recall_unans_prompts.png\", format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456c8aa",
   "metadata": {},
   "source": [
    "### evaluation of GPT-4o mini on 50 manually samples examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04569347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "with open(\"analysis/gpt-eval/ans_responses.json\", \"r\") as f:\n",
    "    ans_responses = json.load(f)\n",
    "with open(\"analysis/gpt-eval/unans_responses.json\", \"r\") as f:\n",
    "    unans_responses = json.load(f)\n",
    "\n",
    "for item in tqdm(ans_responses):\n",
    "    model_response = item[\"response\"]\n",
    "    classification = classify_completion(model_response)\n",
    "    if classification == 1:\n",
    "            tp += 1\n",
    "    else:\n",
    "            fn += 1\n",
    "            print(f\"False negative for answerable response: {model_response}\")\n",
    "for item in tqdm(unans_responses):\n",
    "    model_response = item[\"response\"]\n",
    "    classification = classify_completion(model_response)\n",
    "    if classification == 0:\n",
    "            tn += 1\n",
    "    else:\n",
    "            fp += 1\n",
    "            print(f\"False positive for unanswerable response: {model_response}\")\n",
    "print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv-tova)",
   "language": "python",
   "name": "myenv-tova"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
